# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================

"""Precise dataset classes.

These can use higher precision than float64, but cannot be jit-compiled or
auto-differentiated. For that, see the `JaxDataset` classes. The reason for
having JAX args and return values is to be able to use JAX's datatypes (regular
numpy does not support bfloat16).
"""

from typing import List

import jax.numpy as jnp
import jax.random as jrandom

from evolution.lib.python import printing  # pylint: disable=unused-import
from evolution.lib.python import rng as rng_lib
from evolution.projects.graphs.joy import bfloat16_util
from evolution.projects.graphs.joy import data as data_lib
from evolution.projects.graphs.joy import data_pb2
from evolution.projects.graphs.joy import dataset_spec_pb2
from evolution.projects.graphs.joy import error_util
from evolution.projects.graphs.joy import ground_truth as ground_truth_lib


JnpPreciseFloat = data_lib.JnpFloat


class Dataset:
  """Interface for datasets."""

  @property
  def inputs_dtype(self) -> data_lib.JnpFloatDType:
    """The dtype of the inputs that are produced by this class.

    Does not refer to the dtype of internally-generated labels or error
    computation, which may use higher precision.

    Returns:
      A JNP dtype.
    """
    raise NotImplementedError("Must be implemented by subclass")

  def inputs(self) -> List[jnp.ndarray]:
    """Returns a new set of inputs.

    Returns:
      A list of input arrays. Each item in the list correspond to a graph input.
      A graph with one input (e.g. "x") needs a list of length 1. The elements
      of the array are input values for the different examples. Therefore, all
      arrays must be of the same length (the number of examples). All arrays
      are of the dtype returned by `inputs_dtype`.
    """
    raise NotImplementedError("Must be implemented by subclass")

  def max_relative_error(
      self, inputs: List[jnp.ndarray], predictions: List[jnp.ndarray]
  ) -> JnpPreciseFloat:
    """Returns the max relative error, computing the labels internally.

    Calculating the error with this method allows working in higher-than-machine
    precision. However, this approach will be typically slow as it always
    recalculates the labels internally.

    Args:
      inputs: a list of input arrays, typically originally calculated by the
        this object's `inputs` method. The format is the same as the return
        value of that method.
      predictions: the corresponding outputs, generated by evaluating a graph.
        The fornat is a list of arrays. Each item in the list corresponds to a
        graph output. A graph with one output (e.g. "f") needs a list of length
        1. The elements of the array are output values for the different
        examples. Therefore, all arrays must be of the same length (the number
        of examples). Each element of the arrays is in correspondence with the
        inputs at the same position.

    Returns:
      The error.
    """
    raise NotImplementedError("Must be implemented by subclass.")


def build(spec: dataset_spec_pb2.DatasetSpec) -> Dataset:
  if spec.HasField("fixed"):
    return FixedDataset(spec=spec.fixed)
  elif spec.HasField("random"):
    return RandomDataset(spec=spec.random)
  elif spec.HasField("full_bfloat16"):
    return FullBfloat16Dataset(spec=spec.full_bfloat16)
  else:
    raise NotImplementedError("Unknown dataset.")


class FixedDataset(Dataset):
  """A dataset that always returns the same inputs and labels."""

  def __init__(self, spec: dataset_spec_pb2.FixedDatasetSpec):
    """Initializes the instance."""
    self._spec = spec

    # Calculate the fixed inputs.
    if self._spec.num_inputs <= 0:
      raise ValueError("Missing or invalid inputs.")
    if not self._spec.HasField("inputs_min"):
      raise ValueError("Missing inputs_min.")
    if not self._spec.HasField("inputs_max"):
      raise ValueError("Missing inputs_max.")
    if self._spec.inputs_min > self._spec.inputs_max:
      raise ValueError("Inconsistent input limits.")
    if not self._spec.HasField("inputs_dtype"):
      raise ValueError("Missing inputs_dtype.")
    if self._spec.inputs_dtype == data_pb2.BFLOAT16:
      raise ValueError(
          "Incorrect Dataset class used. Please use FullBfloat16Dataset"
      )

    self._inputs_dtype = data_lib.get_dtype(self._spec.inputs_dtype)
    num_inputs = self._spec.num_inputs
    self._inputs = jnp.linspace(
        start=self._spec.inputs_min,
        stop=self._spec.inputs_max,
        num=num_inputs,
        dtype=self._inputs_dtype,
    )
    if not self._spec.inputs_min_inclusive:
      if num_inputs < 2:
        raise ValueError("Too few points.")
      self._inputs = self._inputs[1:]
      num_inputs -= 1
    if not self._spec.inputs_max_inclusive:
      if num_inputs < 2:
        raise ValueError("Too few points.")
      self._inputs = self._inputs[:-1]
      num_inputs -= 1

    # Prepare structures to calculate the labels and error with each example.
    if not self._spec.HasField("ground_truth"):
      raise ValueError("Missing ground_truth.")
    ground_truth = ground_truth_lib.build(self._spec.ground_truth)
    if self._spec.relative_error_epsilon < 0.0:
      raise ValueError("Found negative relative_error_epsilon.")

    if not self._spec.HasField("measure_ulp"):
      raise ValueError("Missing measure_ulp.")
    if self._spec.measure_ulp:
      if self._spec.HasField("override_ulp_dtype"):
        ulp_dtype = data_lib.get_dtype(self._spec.override_ulp_dtype)
      else:
        ulp_dtype = self._inputs_dtype
    else:
      assert not self._spec.HasField("override_ulp_dtype")
      ulp_dtype = None
    if ulp_dtype is None:
      assert not self._spec.HasField("override_ulp_with_value_at")
      override_ulp_with_value_at = None
    else:
      if self._spec.HasField("override_ulp_with_value_at"):
        override_ulp_with_value_at = self._spec.override_ulp_with_value_at
      else:
        override_ulp_with_value_at = None

    self._error_accumulator = error_util.ErrorAccumulator(
        ground_truth=ground_truth,
        epsilon=self._spec.relative_error_epsilon,
        ulp_dtype=ulp_dtype,
        override_ulp_with_value_at=override_ulp_with_value_at,
    )

  @property
  def inputs_dtype(self) -> data_lib.JnpFloatDType:
    return self._inputs_dtype

  def inputs(self) -> List[jnp.ndarray]:
    """Always returns the same inputs. See base class."""
    return [self._inputs]

  def max_relative_error(
      self, inputs: List[jnp.ndarray], predictions: List[jnp.ndarray]
  ) -> JnpPreciseFloat:
    """Calculates the error. See base class."""
    self._error_accumulator.reset()
    if len(inputs) != 1 or len(predictions) != 1:
      raise NotImplementedError("Not yet implemented.")
    self._error_accumulator.accumulate_examples(
        inputs=inputs[0], predictions=predictions[0]
    )
    return self._error_accumulator.max_relative_error()


class RandomDataset(Dataset):
  """A dataset that returns a random set of inputs and corresponding labels."""

  def __init__(self, spec: dataset_spec_pb2.RandomDatasetSpec):
    """Initializes the instance."""
    self._spec = spec

    if self._spec.num_inputs <= 0:
      raise ValueError("Missing or invalid inputs.")
    if not self._spec.HasField("inputs_min"):
      raise ValueError("Missing inputs_min.")
    if not self._spec.HasField("inputs_max"):
      raise ValueError("Missing inputs_max.")
    if self._spec.inputs_min > self._spec.inputs_max:
      raise ValueError("Inconsistent input limits.")
    if not self._spec.HasField("inputs_dtype"):
      raise ValueError("Missing inputs_dtype.")
    self._inputs_dtype = data_lib.get_dtype(self._spec.inputs_dtype)
    if self._spec.inputs_dtype == data_pb2.BFLOAT16:
      raise ValueError(
          "Incorrect Dataset class used. Please use FullBfloat16Dataset"
      )

    if not self._spec.HasField("ground_truth"):
      raise ValueError("Missing ground_truth.")
    ground_truth = ground_truth_lib.build(self._spec.ground_truth)
    if self._spec.relative_error_epsilon < 0.0:
      raise ValueError("Found negative relative_error_epsilon.")

    if not self._spec.HasField("measure_ulp"):
      raise ValueError("Missing measure_ulp.")
    if self._spec.measure_ulp:
      if self._spec.HasField("override_ulp_dtype"):
        ulp_dtype = data_lib.get_dtype(self._spec.override_ulp_dtype)
      else:
        ulp_dtype = self._inputs_dtype
    else:
      assert not self._spec.HasField("override_ulp_dtype")
      ulp_dtype = None
    if ulp_dtype is None:
      assert not self._spec.HasField("override_ulp_with_value_at")
      override_ulp_with_value_at = None
    else:
      if self._spec.HasField("override_ulp_with_value_at"):
        override_ulp_with_value_at = self._spec.override_ulp_with_value_at
      else:
        override_ulp_with_value_at = None
    self._error_accumulator = error_util.ErrorAccumulator(
        ground_truth=ground_truth,
        epsilon=self._spec.relative_error_epsilon,
        ulp_dtype=ulp_dtype,
        override_ulp_with_value_at=override_ulp_with_value_at,
    )

    if self._spec.HasField("seed") and self._spec.seed > 0:
      self.key = jrandom.PRNGKey(self._spec.seed)
    else:
      self.key = jrandom.PRNGKey(rng_lib.GenerateRNGSeed())

  @property
  def inputs_dtype(self) -> data_lib.JnpFloatDType:
    return self._inputs_dtype

  def generate(self) -> jnp.ndarray:
    """Generates a random set of inputs."""
    self.key, subkey = jrandom.split(self.key)
    num_inputs = self._spec.num_inputs
    inputs = jrandom.uniform(
        key=subkey,
        minval=self._spec.inputs_min,
        maxval=self._spec.inputs_max,
        shape=(num_inputs,),
        dtype=self._inputs_dtype,
    )
    if not self._spec.inputs_min_inclusive:
      if num_inputs < 2:
        raise ValueError("Too few points.")
      inputs = inputs[1:]
      num_inputs -= 1
    if not self._spec.inputs_max_inclusive:
      if num_inputs < 2:
        raise ValueError("Too few points.")
      inputs = inputs[:-1]
      num_inputs -= 1

    return inputs

  def inputs(self) -> List[jnp.ndarray]:
    """Always returns a random set of inputs."""
    return [self.generate()]

  def max_relative_error(
      self, inputs: List[jnp.ndarray], predictions: List[jnp.ndarray]
  ) -> JnpPreciseFloat:
    """Calculates the error. See base class."""
    self._error_accumulator.reset()
    if len(inputs) != 1 or len(predictions) != 1:
      raise NotImplementedError("Not yet implemented.")
    self._error_accumulator.accumulate_examples(
        inputs=inputs[0], predictions=predictions[0]
    )
    return self._error_accumulator.max_relative_error()


class FullBfloat16Dataset(Dataset):
  """A bfloat16 dataset.

  It always returns the same full set of inputs and labels.
  """

  def __init__(self, spec: dataset_spec_pb2.FullBfloat16DatasetSpec):
    """Initializes the instance."""
    self._spec = spec

    # Calculate the fixed inputs.
    if not self._spec.HasField("inputs_min"):
      raise ValueError("Missing inputs_min.")
    if not self._spec.HasField("inputs_max"):
      raise ValueError("Missing inputs_max.")
    if self._spec.inputs_min > self._spec.inputs_max:
      raise ValueError("Inconsistent input limits.")
    self._inputs_dtype = jnp.bfloat16

    self._inputs = bfloat16_util.all_bfloat16_values(
        self._spec.inputs_min,
        self._spec.inputs_max,
        self._spec.skip_every,
        self._spec.inputs_min_inclusive,
        self._spec.inputs_max_inclusive,
    )

    # Prepare structures to calculate the labels and error with each example.
    if not self._spec.HasField("ground_truth"):
      raise ValueError("Missing ground_truth.")
    ground_truth = ground_truth_lib.build(self._spec.ground_truth)
    if self._spec.relative_error_epsilon < 0.0:
      raise ValueError("Found negative relative_error_epsilon.")

    if not self._spec.HasField("measure_ulp"):
      raise ValueError("Missing measure_ulp.")
    if self._spec.measure_ulp:
      if self._spec.HasField("override_ulp_dtype"):
        ulp_dtype = data_lib.get_dtype(self._spec.override_ulp_dtype)
      else:
        ulp_dtype = self._inputs_dtype
    else:
      assert not self._spec.HasField("override_ulp_dtype")
      ulp_dtype = None
    if ulp_dtype is None:
      assert not self._spec.HasField("override_ulp_with_value_at")
      override_ulp_with_value_at = None
    else:
      if self._spec.HasField("override_ulp_with_value_at"):
        override_ulp_with_value_at = self._spec.override_ulp_with_value_at
      else:
        override_ulp_with_value_at = None
    self._error_accumulator = error_util.ErrorAccumulator(
        ground_truth=ground_truth,
        epsilon=self._spec.relative_error_epsilon,
        ulp_dtype=ulp_dtype,
        override_ulp_with_value_at=override_ulp_with_value_at,
    )

  @property
  def inputs_dtype(self) -> data_lib.JnpFloatDType:
    return self._inputs_dtype

  def inputs(self) -> List[jnp.ndarray]:
    """Always returns the same inputs. See base class."""
    return [self._inputs]

  def max_relative_error(
      self, inputs: List[jnp.ndarray], predictions: List[jnp.ndarray]
  ) -> JnpPreciseFloat:
    """Calculates the error. See base class."""
    self._error_accumulator.reset()
    if len(inputs) != 1 or len(predictions) != 1:
      raise NotImplementedError("Not yet implemented.")
    self._error_accumulator.accumulate_examples(
        inputs=inputs[0], predictions=predictions[0]
    )
    return self._error_accumulator.max_relative_error()
